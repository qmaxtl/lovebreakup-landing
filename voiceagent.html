<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Talk to Aira</title>
  <script src="https://unpkg.com/@vapi-ai/web-sdk"></script>
  <script src="https://unpkg.com/@vapi-ai/client-sdk-react/dist/embed/widget.umd.js" async type="text/javascript"></script>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      padding: 40px;
      background: #fff3f8;
    }

    h1 {
      font-size: 28px;
      color: #111;
    }

    #micBtn {
      padding: 15px 25px;
      background: #ec4899;
      color: white;
      font-size: 18px;
      border: none;
      border-radius: 10px;
      cursor: pointer;
      transition: background 0.3s;
    }

    #micBtn:hover {
      background: #db2777;
    }

    #transcript {
      margin-top: 30px;
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px #ddd;
      min-height: 40px;
    }
  </style>
</head>
<body>
  <h1>ğŸ™ï¸ Talk to Aira</h1>
  <p>Click the button below and start speaking.</p>

  <button id="micBtn">ğŸ™ï¸ Tap to Talk</button>

  <div id="transcript"><i>Transcript will appear here...</i></div>

  <!-- VAPI Floating Widget Embed -->
  <vapi-widget
    assistant-id="a039c975-4660-4630-92e3-51582df6b41f"
    public-key="75c64877-597f-4833-ab36-9261f74dc5ea"
    position="bottom-right"
    width="80px"
    height="80px"
    theme="light">
  </vapi-widget>

  <script>
    let vapi;
    let isListening = false;

    const micBtn = document.getElementById('micBtn');
    const transcriptDiv = document.getElementById('transcript');

    async function initVapiAndStart() {
      try {
        // Force microphone access
        await navigator.mediaDevices.getUserMedia({ audio: true });

        // Init Vapi
        if (!vapi) {
          vapi = new Vapi({
            apiKey: "75c64877-597f-4833-ab36-9261f74dc5ea",
            assistant: { id: "a039c975-4660-4630-92e3-51582df6b41f" },
          });

          vapi.on('transcript', (data) => {
            transcriptDiv.innerHTML = `<b>You said:</b><br>${data.transcript}`;
          });

          vapi.on('speech', (data) => {
            console.log("AI is speaking:", data);
          });

          vapi.on('end', () => {
            micBtn.innerText = 'ğŸ™ï¸ Tap to Talk';
            isListening = false;
          });

          vapi.on('error', (err) => {
            transcriptDiv.innerHTML = `<span style="color:red;">Error: ${err.message}</span>`;
            micBtn.innerText = 'ğŸ™ï¸ Tap to Talk';
            isListening = false;
          });
        }

        if (!isListening) {
          vapi.start();
          micBtn.innerText = 'ğŸ›‘ Stop';
          transcriptDiv.innerHTML = `<i>Listening...</i>`;
          isListening = true;
        } else {
          vapi.stop();
          micBtn.innerText = 'ğŸ™ï¸ Tap to Talk';
          isListening = false;
        }
      } catch (err) {
        alert("Microphone access is required to use this feature. Please allow it.");
        console.error("Mic permission error:", err);
      }
    }

    micBtn.addEventListener('click', initVapiAndStart);
  </script>
</body>
</html>
